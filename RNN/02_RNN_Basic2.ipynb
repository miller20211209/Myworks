{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 簡單的 RNN 模型\n",
    "- 與 RNN_Basic.ipynb 的差異在於模型的建立"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 載入相關套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 嵌入層測試\n",
    "- nn.Embedding(num_embedding, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.3802, -0.8863,  1.5304, -1.1923, -0.5984],\n",
      "         [ 0.1555,  0.9591, -0.2323,  0.5426,  0.6681],\n",
      "         [ 0.6925,  1.2024,  0.5174,  0.5054, -0.8329]],\n",
      "\n",
      "        [[-0.5477, -0.0777,  0.1827,  0.0597, -1.0250],\n",
      "         [ 2.4584,  0.9517, -1.7216, -0.7901, -0.8735],\n",
      "         [-1.2764, -0.0849,  0.9628,  0.8394, -0.2454]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.LongTensor([[0, 1, 2], [3, 4, 5]])\n",
    "embeds = nn.Embedding(6, 5)\n",
    "print(embeds(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 顯示嵌入層的起始權重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.3802, -0.8863,  1.5304, -1.1923, -0.5984],\n",
       "        [ 0.1555,  0.9591, -0.2323,  0.5426,  0.6681],\n",
       "        [ 0.6925,  1.2024,  0.5174,  0.5054, -0.8329],\n",
       "        [-0.5477, -0.0777,  0.1827,  0.0597, -1.0250],\n",
       "        [ 2.4584,  0.9517, -1.7216, -0.7901, -0.8735],\n",
       "        [-1.2764, -0.0849,  0.9628,  0.8394, -0.2454]], requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 輸入改為 1 ~ 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.1017, -0.4459, -1.2649,  1.1693, -0.4569],\n",
      "         [-1.2596,  2.0345, -1.0866,  0.0611, -1.1104],\n",
      "         [-0.5033,  0.4887,  0.8963, -2.0216, -1.7435]],\n",
      "\n",
      "        [[ 0.9040,  1.1129,  1.0517,  0.1697,  0.6378],\n",
      "         [-0.8464,  0.3780,  1.4029, -0.6041,  0.4422],\n",
      "         [-0.2779, -0.3103,  0.9005, -0.5982,  0.3340]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\n",
    "embeds = nn.Embedding(7, 5)\n",
    "print(embeds(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.8223,  1.6487, -0.3123,  0.1081,  0.3252],\n",
      "         [ 0.6250, -0.8517,  0.7489,  0.3979,  0.6393],\n",
      "         [ 0.2944, -0.0113, -2.0318,  0.9561,  0.9945]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[[ 1.4096,  0.2583, -0.2184, -0.8004, -0.0685],\n",
      "         [-0.9140, -1.9244,  0.1771,  0.2556,  0.3434]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.8223,  1.6487, -0.3123,  0.1081,  0.3252],\n",
       "        [ 0.6250, -0.8517,  0.7489,  0.3979,  0.6393],\n",
       "        [ 0.2944, -0.0113, -2.0318,  0.9561,  0.9945],\n",
       "        [ 1.4096,  0.2583, -0.2184, -0.8004, -0.0685],\n",
       "        [-0.9140, -1.9244,  0.1771,  0.2556,  0.3434],\n",
       "        [ 2.2102, -0.4404,  0.8149,  0.0420, -1.4677]], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds = nn.Embedding(6, 5)\n",
    "x1 = torch.LongTensor([[0, 1, 2]])\n",
    "x2 = torch.LongTensor([[3, 4]])\n",
    "print(embeds(x1))\n",
    "print(embeds(x2))\n",
    "embeds.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3085, -0.1052,  0.7499,  1.2802, -0.5273],\n",
      "         [ 1.5887,  0.3831,  0.7766,  1.3853,  0.5812],\n",
      "         [ 1.1188,  2.2343, -0.7616, -1.3601, -0.3304]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[[-2.4371, -1.8138,  0.1802,  0.1987,  0.4644],\n",
      "         [ 0.2488, -0.9374,  0.1968,  0.0477, -0.0340]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[[-2.4371, -1.8138,  0.1802,  0.1987,  0.4644],\n",
      "         [ 0.2488, -0.9374,  0.1968,  0.0477, -0.0340]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.3085, -0.1052,  0.7499,  1.2802, -0.5273],\n",
       "        [ 1.5887,  0.3831,  0.7766,  1.3853,  0.5812],\n",
       "        [ 1.1188,  2.2343, -0.7616, -1.3601, -0.3304],\n",
       "        [-2.4371, -1.8138,  0.1802,  0.1987,  0.4644],\n",
       "        [ 0.2488, -0.9374,  0.1968,  0.0477, -0.0340],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds = nn.Embedding(6, 5, 5)\n",
    "x1 = torch.LongTensor([[0, 1, 2]])\n",
    "x2 = torch.LongTensor([[3, 4]])\n",
    "x3 = torch.LongTensor([[3, 4]])\n",
    "print(embeds(x1))\n",
    "print(embeds(x2))\n",
    "print(embeds(x3))\n",
    "embeds.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 英文單字輸入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3085, -0.1052,  0.7499,  1.2802, -0.5273]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 測試資料\n",
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "\n",
    "# 詞彙表 (vocabulary) 含兩個單字, 轉換為 5 維向量\n",
    "enbeds = nn.Embedding(2, 5)\n",
    "\n",
    "# 測試 hello\n",
    "lookup_tensor = torch.LongTensor([word_to_ix[\"hello\"]])\n",
    "hello_embed = embeds(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN 層的測試\n",
    "- 輸入為二維 (batch_first 是 default, 預設 batch_first = False, => 輸入是二維(不含 batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:  torch.Size([5, 20])\n",
      "hidden layer dim. : torch.Size([2, 20])\n"
     ]
    }
   ],
   "source": [
    "# 測試資料\n",
    "# (seq_len, input_size)\n",
    "input = torch.randn(5, 10)\n",
    "\n",
    "# 建立 RNN 物件\n",
    "# RNN 可以單獨使用 (亦可接在嵌入層後)\n",
    "# (input_size, hidden_size, num_layers)\n",
    "rnn = nn.RNN(10, 20, 2)\n",
    "\n",
    "# RNN 處理\n",
    "# output : (seq_len, hidden_size)\n",
    "# hidden : (num_layer, hidden_size)\n",
    "output, hn = rnn(input)\n",
    "\n",
    "# 顯示輸出及隱藏層的維度\n",
    "print(\"output: \", output.shape)\n",
    "print(\"hidden layer dim. :\", hn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 輸入為三維 (L, batch_size, $H_{in}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:  torch.Size([5, 4, 20])\n",
      "hidden layer dim. : torch.Size([2, 4, 20])\n"
     ]
    }
   ],
   "source": [
    "# 測試資料\n",
    "# (seq_len, batch_size, input_size)\n",
    "input = torch.randn(5, 4, 10)\n",
    "\n",
    "# 建立 RNN 物件\n",
    "# RNN 可以單獨使用 (亦可接在嵌入層後)\n",
    "# (input_size, hidden_size, num_layers)\n",
    "rnn = nn.RNN(10, 20, 2)\n",
    "\n",
    "# RNN 處理\n",
    "# output : (seq_len, batch_size, hidden_size)\n",
    "# hidden : (num_layer, batch_size, hidden_size)\n",
    "output, hn = rnn(input)\n",
    "\n",
    "# 顯示輸出及隱藏層維度\n",
    "print(\"output: \", output.shape)\n",
    "print(\"hidden layer dim. :\", hn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN 的輸入可以有初始的隱藏層狀態 (h0), h0 最後一維需等於 $H_{out}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:  torch.Size([5, 3, 20])\n",
      "hidden layer dim. : torch.Size([2, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "# 測試資料\n",
    "# (seq_len, batch_size, input_size)\n",
    "input = torch.randn(5, 3, 10)\n",
    "\n",
    "# 建立 RNN 物件\n",
    "# RNN 可以單獨使用 (亦可接在嵌入層後)\n",
    "# (input_size, hidden_size, num_layers)\n",
    "rnn = nn.RNN(10, 20, 2)\n",
    "\n",
    "# 隱藏層的輸入\n",
    "# (num_layers, batch_size, hidden_size)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "\n",
    "# RNN 處理\n",
    "# output : (seq_len, batch_size, hidden_size)\n",
    "# hidden : (num_layer, batch_size, hidden_size)\n",
    "output, hn = rnn(input, h0)\n",
    "\n",
    "# 顯示輸出及隱藏層的維度\n",
    "print(\"output: \", output.shape)\n",
    "print(\"hidden layer dim. :\", hn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 接著介紹 PyTorch 前置處理功能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['could', 'have', 'done', 'better']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "text = \"Could have done better\"\n",
    "tokenizer(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 詞彙表處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('could', 1), ('have', 1), ('done', 1), ('better', 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.vocab import vocab\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "# BOW 統計\n",
    "counter = Counter(tokenizer(text))\n",
    "\n",
    "# 依出現次數降冪排列\n",
    "sort_by_freq_tuples = sorted(counter.items(),\n",
    "                            key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# 建立詞彙字典\n",
    "ordered_dict = OrderedDict(sort_by_freq_tuples)\n",
    "print(ordered_dict)\n",
    "\n",
    "# 建立詞彙表物件, 並加一個未知單字 (unknow) 的索引值\n",
    "vocab_object = torchtext.vocab.vocab(ordered_dict, specials = [\"<unk>\"])\n",
    "\n",
    "# 設定詞彙表預設值為未知單字(unknown)的索引值\n",
    "vocab_object.set_default_index(vocab_object[\"<unk>\"])\n",
    "\n",
    "# 測試\n",
    "vocab_object['done']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 取得詞彙表的所有單字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', 'could', 'have', 'done', 'better']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_object.get_itos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 取得詞彙表的單字個數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_object.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 字串標點符號"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "string.punctuation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('dp1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11392ef459069d601d11ff7ceba327d19eb4b45e7e3534b48cc5347edbf5a449"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
